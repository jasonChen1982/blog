---
title: 光线追踪
date: 2015-10-02
---


可能每个喜欢计算机图形学的程序员都有一个实现光线追踪的梦，本文将利用`canvas2d`的api来实现一个简单的js版的光线追踪。

## 什么叫光线追踪

光线追踪可以理解成是一种以基于光线传播的形势来计算和渲染三维场景中物体可见表面的颜色值，它的处理单元可以理解成光线`光线(ray)` 。

与我们常见的`D3D` 、`opengl` 等API不同，这些我们熟知的方法一般都运用于`实时的图形渲染场景(real-time rendering)`，大都采用`光栅化(rasterization)`机制来渲染大量的三角形，任何三维物体的最小组成单位是三角形，通过在三角形上进行各种向量的插值、纹理采样来绘制三维物体。并且这种基于光栅化机制的渲染方案只支持局部光照系统，不能很简单、干脆的实现类似 `阴影(shadow)`、`反射(reflection)`、`折射(refraction)` 等现实场景中的常见效果的渲染。只能通过类似`阴影贴图(shadow mapping)`、`环境贴图(environment mapping)`等技术来曲线的实现。

而光线追踪则不同，光线追踪的从观察点向三维场景中发射光线 ，来捕获并计算场景中物体可见部位的颜色，并可以在实现原理上通过加入相应的阴影、反射、折射的计算公式来轻松的搞定这些效果。本文讲介绍光线追踪解决方案中的一种：`光线追踪(ray tracing)` 

## 原理 

之前说了，光线追踪与我们熟悉的光栅化渲染方式不一样，我们熟悉的光栅化渲染方式的原理简单来说是把所有的三维物体以三角形为单位来近似的描述三维物体的表面，然后经过对三角形的各种仿射变换、`backface` 检测、插值和纹理采样、 光栅化、深度缓冲区的过滤、模版缓冲区、合成计算将最终的颜色呈现在屏幕上。

光线追踪是一个完全不一样的思路，光线追踪是利用了物理中的`光路的可逆性`来实现三维场景的渲染，主要原理是从观察者的眼睛的位置发射一条经过投影平面的射线，然后计算这条射线与场景中的三维物体的相交，再根据该相交点的材质、法线、反射、折射、以及阴影检测来综合计算该点的颜色值。原理图如下：

![ray tracing diagram][ray-tracing]



知道了原理，我们就可以通过将三维场景的坐标点投影到二维平面也就是设备的屏幕上，从而渲染出在当前这个视角下的画面了。关于投影的问题，其实我们不需要像光栅化机制的渲染系统一样非得利用投影矩阵来完成投影计算，我们可以直接利用向量的计算就能达到这样的目的，这也是ray tracing基于光线的渲染给我们带来的小小的便利，并且在以后的更复杂的光线追踪中也很少用到矩阵的操作（ray tracing的方式从原理上避免了这些）



## 照相机

照相机的作用就是帮我们处理好三维空间到二维平面的投影关系，让我们只需要告诉照相机我要渲染哪个像素点，照相机就会给我们生成一条射线，然后我们用这条射线到场景里面与物体计算相交，得出相交得最近的物体，让后根据法线、光源、反射光源来计算该点的颜色值。照相机的代码如下：

```js
import { Vector3 } from './math/Vector3';
import { Ray } from './math/Ray';
import { _Math } from './math/Math';

function Camera(eye, fov, front, up) {
  this.eye = eye;
  this.front = front;
  this.uper = up;
  this.fov = fov;
  this.init();
}
Camera.prototype = {
  constructor: Camera,
  init: function () {
    this.right = new Vector3().crossVectors(this.front, this.uper);
    this.up = new Vector3().crossVectors(this.right, this.front);
    this.fovScale = Math.tan(this.fov * 0.5 * _Math.DTR) * 2;
  },
  getRay: function (x, y) {
    var r = new Vector3().copy(this.right).multiplyScalar((x - 0.5) * this.fovScale);
    var u = new Vector3().copy(this.up).multiplyScalar((y - 0.5) * this.fovScale);
    var d = new Vector3().copy(this.front).add(r).add(u).normalize();

    return new Ray(this.eye, d);
  }
};
```

`getRay`方法就是生成射线的方法，接收二维投影平面的`x` `y` 坐标值。为了方便和与常用的坐标系保持一致，我们需要对二维投影平面的坐标系做一个定义，这里我们采用`笛卡尔坐标系`。中心点为`(0,0)`点，然后将 `x` `y` 取值区间设置在[-1,1]之间方便后续利用视角`fov`的概念来描述视野的宽广度。

因此对于像素点 `x` `y` ，我们可以得到光线`ray`:

```js
const r = this.right.multiply((x - 0.5) * this.fovScale);
const u = this.up.multiply((y - 0.5) * this.fovScale);
const ray = new Ray3(this.eye, this.front.add(r).add(u).normalize());
```

其中`this.eye`是光线的起始位置，`this.front.add(r).add(u).normalize()`是光线的方向向量。`this.fovScale`是通过`fov`计算而来的缩放标量。



## 计算过程

有了这个射线我们就可以与场景中的物体进行相交监测，并找出最近的那个相交的物体，进而计算出该点的颜色值。这个过程是以个纯数学过程，例如利用向量的`点乘`和`叉乘`。

结合上面介绍的，我们来一步一步实现投影平面上各个像素颜色的计算（也可以称为渲染过程）。

假如我们要渲染而为投影屏幕上中心上的那个点

[ray-tracing]: https://jasonchen1982.github.io/blog/pictures/raytracing/raytracediagram.png	"ray tracing diagram"



